import pytest
import json
from pathlib import Path
import tempfile
import asyncio
import os
import uuid
import logging
import psycopg2

from src.ocr.extraction import (
    trigger_extraction,
    perform_ocr,
    postprocess_ocr,
    run_llm_extraction,
    generate_visualization,
    save_extracted_field
)
from src.creditsystem.storage import Stage, get_storage
from src.ocr.storage import read_ocr_results_from_bucket
from src.llm.client import OllamaClient
from src.config import AppConfig

logger = logging.getLogger(__name__)


@pytest.fixture
def setup_database_env(dms_mock_environment):
    """Set up database environment variables for status updates."""
    # Set environment variables for database connection
    os.environ["POSTGRES_HOST"] = "localhost"
    os.environ["POSTGRES_PORT"] = str(dms_mock_environment.postgres_port)
    os.environ["POSTGRES_DB"] = "dms_meta"
    os.environ["POSTGRES_USER"] = "dms"
    os.environ["POSTGRES_PASSWORD"] = "dms"
    
    yield
    
    # Clean up environment variables
    for var in ["POSTGRES_HOST", "POSTGRES_PORT", "POSTGRES_DB", "POSTGRES_USER", "POSTGRES_PASSWORD"]:
        os.environ.pop(var, None)


@pytest.fixture
def llm_client():
    """Create a test LLM client."""
    app_config = AppConfig("config")
    return OllamaClient(
        base_url=app_config.generative_llm.url,
        model_name=app_config.generative_llm.model_name
    )


class TestTriggerExtraction:
    """Test trigger_extraction function."""
    
    def test_trigger_extraction_creates_job(self, setup_database_env):
        """Test that trigger_extraction creates a job in the database."""
        test_document_id = str(uuid.uuid4())
        
        # First, create a document record in the database
        connection = psycopg2.connect(
            host=os.environ["POSTGRES_HOST"],
            port=os.environ["POSTGRES_PORT"],
            database=os.environ["POSTGRES_DB"],
            user=os.environ["POSTGRES_USER"],
            password=os.environ["POSTGRES_PASSWORD"]
        )
        
        try:
            with connection.cursor() as cursor:
                cursor.execute(
                    """
                    INSERT INTO Dokument (dokument_id, dokumententyp, pfad_dms, quelle_dateiname, hash_sha256)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (test_document_id, "credit_request", "/test/path", "test_document.pdf", "a" * 64)
                )
            connection.commit()
        finally:
            connection.close()
        
        # Trigger extraction
        result = trigger_extraction(test_document_id)
        
        # Verify result - trigger_extraction returns the job ID, not document ID
        assert result is not None, "Extraction should return a job ID"
        assert len(result) > 0, "Job ID should not be empty"
        
        # Verify job was created in database
        connection = psycopg2.connect(
            host=os.environ["POSTGRES_HOST"],
            port=os.environ["POSTGRES_PORT"],
            database=os.environ["POSTGRES_DB"],
            user=os.environ["POSTGRES_USER"],
            password=os.environ["POSTGRES_PASSWORD"]
        )
        
        try:
            with connection.cursor() as cursor:
                cursor.execute(
                    """
                    SELECT status, fehlermeldung 
                    FROM Extraktionsauftrag 
                    WHERE dokument_id = %s 
                    ORDER BY erstellt_am DESC 
                    LIMIT 1
                    """,
                    (test_document_id,)
                )
                result_row = cursor.fetchone()
                
                assert result_row is not None, "Job was not created in database"
                status, fehlermeldung = result_row
                assert status == "Extraktion ausstehend", f"Expected 'Extraktion ausstehend', got '{status}'"
                assert "Job created" in fehlermeldung, f"Expected job creation message, got '{fehlermeldung}'"
        finally:
            connection.close()

    def test_trigger_extraction_handles_multiple_calls(self, setup_database_env):
        """Test that trigger_extraction handles multiple calls for the same document."""
        test_document_id = str(uuid.uuid4())
        
        # First, create a document record in the database
        connection = psycopg2.connect(
            host=os.environ["POSTGRES_HOST"],
            port=os.environ["POSTGRES_PORT"],
            database=os.environ["POSTGRES_DB"],
            user=os.environ["POSTGRES_USER"],
            password=os.environ["POSTGRES_PASSWORD"]
        )
        
        try:
            with connection.cursor() as cursor:
                cursor.execute(
                    """
                    INSERT INTO Dokument (dokument_id, dokumententyp, pfad_dms, quelle_dateiname, hash_sha256)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (test_document_id, "credit_request", "/test/path", "test_document.pdf", "a" * 64)
                )
            connection.commit()
        finally:
            connection.close()
        
        # Trigger extraction multiple times
        result1 = trigger_extraction(test_document_id)
        result2 = trigger_extraction(test_document_id)
        
        # Verify results - both should return job IDs
        assert result1 is not None, "First extraction should return a job ID"
        assert result2 is not None, "Second extraction should return a job ID"
        assert len(result1) > 0, "First job ID should not be empty"
        assert len(result2) > 0, "Second job ID should not be empty"
        
        # Verify multiple jobs were created in database
        connection = psycopg2.connect(
            host=os.environ["POSTGRES_HOST"],
            port=os.environ["POSTGRES_PORT"],
            database=os.environ["POSTGRES_DB"],
            user=os.environ["POSTGRES_USER"],
            password=os.environ["POSTGRES_PASSWORD"]
        )
        
        try:
            with connection.cursor() as cursor:
                cursor.execute(
                    """
                    SELECT COUNT(*) 
                    FROM Extraktionsauftrag 
                    WHERE dokument_id = %s
                    """,
                    (test_document_id,)
                )
                count = cursor.fetchone()[0]
                assert count >= 2, f"Expected at least 2 jobs, got {count}"
        finally:
            connection.close()

    def test_trigger_extraction_handles_database_failure(self, setup_database_env):
        """Test that trigger_extraction handles database connection failures gracefully."""
        test_document_id = str(uuid.uuid4())
        
        # Temporarily remove database environment variables to simulate connection failure
        original_host = os.environ.pop("POSTGRES_HOST", None)
        original_port = os.environ.pop("POSTGRES_PORT", None)
        
        try:
            # This should not raise an exception
            result = trigger_extraction(test_document_id)
            assert result is not None, "Extraction should return a job ID even with database failure"
            assert len(result) > 0, "Job ID should not be empty"
        finally:
            # Restore environment variables
            if original_host:
                os.environ["POSTGRES_HOST"] = original_host
            if original_port:
                os.environ["POSTGRES_PORT"] = original_port


class TestPerformOcr:
    """Test perform_ocr function."""
    
    def test_performs_ocr_on_document(self, setup_database_env):
        """Test that perform_ocr performs OCR on a document."""
        test_document_id = str(uuid.uuid4())
        
        # First trigger extraction to set up the document
        trigger_extraction(test_document_id)
        
        # Perform OCR
        ocr_results = perform_ocr(test_document_id)
        
        # Verify OCR results
        assert ocr_results is not None
        assert "extracted_lines" in ocr_results
        assert "azure_raw_result" in ocr_results
        assert ocr_results["document_id"] == test_document_id

    def test_raises_file_not_found_when_raw_pdf_missing(self, setup_database_env):
        """Test that perform_ocr raises FileNotFoundError when raw PDF is missing."""
        test_document_id = str(uuid.uuid4())
        
        with pytest.raises(FileNotFoundError):
            perform_ocr(test_document_id)


class TestPostprocessOcr:
    """Test postprocess_ocr function."""
    
    def test_postprocesses_ocr_results(self, setup_database_env):
        """Test that postprocess_ocr post-processes OCR results."""
        test_document_id = str(uuid.uuid4())
        
        # Set up the pipeline
        trigger_extraction(test_document_id)
        perform_ocr(test_document_id)
        
        # Post-process OCR
        cleaned_results = postprocess_ocr(test_document_id)
        
        # Verify cleaned results
        assert cleaned_results is not None
        assert "normalized_lines" in cleaned_results
        assert "original_lines" in cleaned_results
        assert cleaned_results["document_id"] == test_document_id
        assert "processing_metadata" in cleaned_results

    def test_raises_file_not_found_when_raw_ocr_missing(self, setup_database_env):
        """Test that postprocess_ocr raises FileNotFoundError when raw OCR is missing."""
        test_document_id = str(uuid.uuid4())
        
        with pytest.raises(FileNotFoundError):
            postprocess_ocr(test_document_id)


class TestRunLlmExtraction:
    """Test run_llm_extraction function."""
    
    @pytest.mark.asyncio
    async def test_runs_llm_extraction(self, llm_client, setup_database_env):
        """Test that run_llm_extraction runs LLM extraction."""
        test_document_id = str(uuid.uuid4())
        
        # Set up the pipeline
        trigger_extraction(test_document_id)
        perform_ocr(test_document_id)
        postprocess_ocr(test_document_id)
        
        # Run LLM extraction
        extracted_fields_result = await run_llm_extraction(test_document_id)
        
        # Verify LLM results
        assert extracted_fields_result is not None
        assert "extracted_fields" in extracted_fields_result
        assert "missing_fields" in extracted_fields_result
        assert extracted_fields_result["document_id"] == test_document_id

    @pytest.mark.asyncio
    async def test_raises_file_not_found_when_clean_ocr_missing(self, llm_client, setup_database_env):
        """Test that run_llm_extraction raises FileNotFoundError when clean OCR is missing."""
        test_document_id = str(uuid.uuid4())
        
        with pytest.raises(FileNotFoundError):
            await run_llm_extraction(test_document_id)


class TestGenerateVisualization:
    """Test generate_visualization function."""
    
    def test_generates_visualization(self, setup_database_env):
        """Test that generate_visualization generates visualization."""
        test_document_id = str(uuid.uuid4())
        
        # Set up the pipeline
        trigger_extraction(test_document_id)
        perform_ocr(test_document_id)
        postprocess_ocr(test_document_id)
        
        # Generate visualization
        visualization_path = generate_visualization(test_document_id)
        
        # Verify visualization was created in blob storage
        assert visualization_path is not None
        assert visualization_path.startswith("credit-docs-annotated/")
        
        # Check that the visualization exists in blob storage
        storage_client = get_storage()
        visualization_data = storage_client.download_blob(test_document_id, Stage.ANNOTATED, ".png")
        assert visualization_data is not None, "Visualization not found in blob storage"

    def test_raises_file_not_found_when_original_pdf_missing(self, setup_database_env):
        """Test that generate_visualization raises FileNotFoundError when original PDF is missing."""
        test_document_id = str(uuid.uuid4())
        
        with pytest.raises(FileNotFoundError):
            generate_visualization(test_document_id)

    def test_raises_file_not_found_when_clean_ocr_missing(self, setup_database_env):
        """Test that generate_visualization raises FileNotFoundError when clean OCR is missing."""
        test_document_id = str(uuid.uuid4())
        
        # Create the document but don't run OCR
        trigger_extraction(test_document_id)
        
        with pytest.raises(FileNotFoundError):
            generate_visualization(test_document_id)


class TestSaveExtractedField:
    """Test save_extracted_field function."""
    
    def test_saves_field_to_database_with_basic_data(self, setup_database_env):
        """Test that save_extracted_field saves basic field data to database."""
        test_document_id = str(uuid.uuid4())
        test_field_name = "company_name"
        test_field_value = "Demo Tech GmbH"
        
        # First create a document record in the database
        from src.ocr.extraction import _get_database_connection
        connection = _get_database_connection()
        if connection is not None:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(
                        """
                        INSERT INTO Dokument (
                            dokument_id, pfad_dms, dokumententyp, hash_sha256, quelle_dateiname, 
                            textextraktion_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (test_document_id, "raw/test.pdf", "Kreditantrag", "a" * 64, "test.pdf", 
                         "nicht bereit")
                    )
                    connection.commit()
            except Exception as e:
                logger.warning(f"Failed to create document record: {e}")
        
        # Save the field
        save_extracted_field(
            document_id=test_document_id,
            field_name=test_field_name,
            value=test_field_value
        )
        
        # Verify the field was saved to the database
        if connection is not None:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(
                        """
                        SELECT feldname, wert, konfidenzscore
                        FROM ExtrahierteDaten
                        WHERE dokument_id = %s AND feldname = %s
                        """,
                        (test_document_id, test_field_name)
                    )
                    result = cursor.fetchone()
                    
                    assert result is not None, "Field was not saved to database"
                    assert result[0] == test_field_name, "Field name mismatch"
                    assert result[1] == test_field_value, "Field value mismatch"
                    assert result[2] is None, "Confidence should be None for basic data"
            except Exception as e:
                logger.warning(f"Failed to verify field save: {e}")
                # If database is not available, just verify the function doesn't raise an exception
                assert True
    
    def test_saves_field_with_position_and_confidence(self, setup_database_env):
        """Test that save_extracted_field saves field data with position and confidence."""
        test_document_id = str(uuid.uuid4())
        test_field_name = "purchase_price"
        test_field_value = "500.000 â‚¬"
        test_position = {"x": 100, "y": 200, "width": 150, "height": 30}
        test_confidence = 0.95
        
        # First create a document record in the database
        from src.ocr.extraction import _get_database_connection
        connection = _get_database_connection()
        if connection is not None:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(
                        """
                        INSERT INTO Dokument (
                            dokument_id, pfad_dms, dokumententyp, hash_sha256, quelle_dateiname, 
                            textextraktion_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (test_document_id, "raw/test.pdf", "Kreditantrag", "a" * 64, "test.pdf", 
                         "nicht bereit")
                    )
                    connection.commit()
            except Exception as e:
                logger.warning(f"Failed to create document record: {e}")
        
        # Save the field with position and confidence
        save_extracted_field(
            document_id=test_document_id,
            field_name=test_field_name,
            value=test_field_value,
            position=test_position,
            confidence=test_confidence
        )
        
        # Verify the field was saved to the database with position and confidence
        if connection is not None:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(
                        """
                        SELECT feldname, wert, position_im_dokument, konfidenzscore
                        FROM ExtrahierteDaten
                        WHERE dokument_id = %s AND feldname = %s
                        """,
                        (test_document_id, test_field_name)
                    )
                    result = cursor.fetchone()
                    
                    assert result is not None, "Field was not saved to database"
                    assert result[0] == test_field_name, "Field name mismatch"
                    assert result[1] == test_field_value, "Field value mismatch"
                    # PostgreSQL JSONB returns dict directly, not JSON string
                    position_data = result[2] if isinstance(result[2], dict) else json.loads(result[2]) if result[2] else None
                    assert position_data == test_position, "Position mismatch"
                    # Handle Decimal type from database
                    confidence_value = float(result[3]) if result[3] is not None else None
                    assert confidence_value == test_confidence, "Confidence mismatch"
            except Exception as e:
                logger.warning(f"Failed to verify field save: {e}")
                # If database is not available, just verify the function doesn't raise an exception
                assert True
    
    def test_handles_none_values_gracefully(self, setup_database_env):
        """Test that save_extracted_field handles None values gracefully."""
        test_document_id = str(uuid.uuid4())
        test_field_name = "missing_field"
        test_field_value = None
        
        # First create a document record in the database
        from src.ocr.extraction import _get_database_connection
        connection = _get_database_connection()
        if connection is not None:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(
                        """
                        INSERT INTO Dokument (
                            dokument_id, pfad_dms, dokumententyp, hash_sha256, quelle_dateiname, 
                            textextraktion_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (test_document_id, "raw/test.pdf", "Kreditantrag", "a" * 64, "test.pdf", 
                         "nicht bereit")
                    )
                    connection.commit()
            except Exception as e:
                logger.warning(f"Failed to create document record: {e}")
        
        # Save the field with None value
        save_extracted_field(
            document_id=test_document_id,
            field_name=test_field_name,
            value=test_field_value
        )
        
        # Verify the field was saved to the database with None value
        if connection is not None:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(
                        """
                        SELECT feldname, wert, konfidenzscore
                        FROM ExtrahierteDaten
                        WHERE dokument_id = %s AND feldname = %s
                        """,
                        (test_document_id, test_field_name)
                    )
                    result = cursor.fetchone()
                    
                    assert result is not None, "Field was not saved to database"
                    assert result[0] == test_field_name, "Field name mismatch"
                    assert result[1] is None, "Field value should be None"
                    assert result[2] is None, "Confidence should be None"
            except Exception as e:
                logger.warning(f"Failed to verify field save: {e}")
                # If database is not available, just verify the function doesn't raise an exception
                assert True


class TestCompletePipeline:
    """Test the complete extraction pipeline."""
    
    @pytest.mark.asyncio
    async def test_complete_extraction_pipeline(self, llm_client, setup_database_env):
        """Test complete extraction pipeline from start to finish."""
        test_document_id = str(uuid.uuid4())
        
        # Step 1: Trigger extraction
        job_id = trigger_extraction(test_document_id)
        assert job_id is not None
        
        # Step 2: Perform OCR
        ocr_results = perform_ocr(test_document_id)
        assert ocr_results is not None
        assert "extracted_lines" in ocr_results
        
        # Step 3: Post-process OCR
        cleaned_results = postprocess_ocr(test_document_id)
        assert cleaned_results is not None
        assert "normalized_lines" in cleaned_results
        
        # Step 4: Run LLM extraction
        extracted_fields_result = await run_llm_extraction(test_document_id)
        assert extracted_fields_result is not None
        assert "extracted_fields" in extracted_fields_result
        
        # Step 5: Generate visualization
        visualization_path = generate_visualization(test_document_id)
        assert visualization_path is not None
        assert visualization_path.startswith("credit-docs-annotated/")
        
        logger.info(f"Complete extraction pipeline completed successfully for document {test_document_id}")
        
        # Verify all stages have data in storage
        storage_client = get_storage()
        
        # Raw PDF
        raw_pdf = storage_client.download_blob(test_document_id, Stage.RAW, ".pdf")
        assert raw_pdf is not None
        
        # Raw OCR
        raw_ocr = read_ocr_results_from_bucket(test_document_id)
        assert raw_ocr is not None
        
        # Clean OCR
        clean_ocr = storage_client.download_blob(test_document_id, Stage.OCR_CLEAN, ".json")
        assert clean_ocr is not None
        
        # LLM results
        llm_results = storage_client.download_blob(test_document_id, Stage.LLM, ".json")
        assert llm_results is not None
        
        # Visualization
        visualization = storage_client.download_blob(test_document_id, Stage.ANNOTATED, ".png")
        assert visualization is not None
        
        # Verify data consistency
        clean_ocr_data = json.loads(clean_ocr.decode('utf-8'))
        llm_data = json.loads(llm_results.decode('utf-8'))
        
        assert clean_ocr_data["document_id"] == test_document_id
        assert llm_data["document_id"] == test_document_id 